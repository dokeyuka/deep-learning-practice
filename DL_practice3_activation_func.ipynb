{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL_practice3_activation_func.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyPIRfPXc6PhSsuq9enZSJtu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dokeyuka/deep-learning-practice/blob/main/DL_practice3_activation_func.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 活性化関数の工夫\n",
        "### 出力層における活性化関数 = 確率を出力する関数でなければならない(Sigmoid, softmax)\n",
        "### 隠れ層は別の関数でもよい→勾配消失問題を解決\n"
      ],
      "metadata": {
        "id": "6zxu0SUQqJRq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 tanh\n",
        "### Sigmoidよりaccuracy上がった！！\n",
        "0.861→0.953"
      ],
      "metadata": {
        "id": "VXNpwHnOrBid"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BSKiGmDlqA63",
        "outputId": "09996d31-ed2f-49b5-8f45-a35562289a0e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, loss: 1.835, a㏄:0.562\n",
            "epoch:2, loss: 0.755, a㏄:0.809\n",
            "epoch:3, loss: 0.487, a㏄:0.870\n",
            "epoch:4, loss: 0.402, a㏄:0.889\n",
            "epoch:5, loss: 0.363, a㏄:0.897\n",
            "epoch:6, loss: 0.339, a㏄:0.903\n",
            "epoch:7, loss: 0.322, a㏄:0.908\n",
            "epoch:8, loss: 0.308, a㏄:0.912\n",
            "epoch:9, loss: 0.297, a㏄:0.915\n",
            "epoch:10, loss: 0.287, a㏄:0.917\n",
            "epoch:11, loss: 0.278, a㏄:0.919\n",
            "epoch:12, loss: 0.270, a㏄:0.922\n",
            "epoch:13, loss: 0.263, a㏄:0.924\n",
            "epoch:14, loss: 0.256, a㏄:0.926\n",
            "epoch:15, loss: 0.249, a㏄:0.928\n",
            "epoch:16, loss: 0.243, a㏄:0.930\n",
            "epoch:17, loss: 0.236, a㏄:0.931\n",
            "epoch:18, loss: 0.230, a㏄:0.933\n",
            "epoch:19, loss: 0.224, a㏄:0.935\n",
            "epoch:20, loss: 0.217, a㏄:0.937\n",
            "epoch:21, loss: 0.211, a㏄:0.939\n",
            "epoch:22, loss: 0.205, a㏄:0.941\n",
            "epoch:23, loss: 0.199, a㏄:0.942\n",
            "epoch:24, loss: 0.193, a㏄:0.945\n",
            "epoch:25, loss: 0.187, a㏄:0.946\n",
            "epoch:26, loss: 0.181, a㏄:0.948\n",
            "epoch:27, loss: 0.176, a㏄:0.950\n",
            "epoch:28, loss: 0.170, a㏄:0.951\n",
            "epoch:29, loss: 0.165, a㏄:0.953\n",
            "epoch:30, loss: 0.160, a㏄:0.955\n",
            "epoch:30, loss: 0.163, acc:0.953\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optimizers\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''\n",
        "  (1) modelの実装\n",
        "'''\n",
        "class DNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.a1 = nn.Tanh()\n",
        "    self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.a2 = nn.Tanh()\n",
        "    self.l3 = nn.Linear(hidden_dim,hidden_dim)\n",
        "    self.a3 = nn.Tanh()\n",
        "    self.l4 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    self.layers = [self.l1, self.a1,\n",
        "                    self.l2, self.a2,\n",
        "                    self.l3, self.a3,\n",
        "                    self.l4]\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  np.random.seed(123)\n",
        "  #torch用の乱数シード\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  #deviceに実行環境を格納して同じコードでCPUでもGPUでも対応できるように\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "'''\n",
        "  1 データの準備\n",
        "'''\n",
        "root = os.path.join('~', '.torch', 'mnist')\n",
        "#numpyをTテンensorに変換し、さらにTensorの次元を(28，28)から(784、)に変換\n",
        "transform = transforms.Compose([transforms.ToTensor(), lambda x: x.view(-1)])\n",
        "\n",
        "mnist_train = datasets.MNIST(root = root,\n",
        "                             download = True,\n",
        "                             train = True,\n",
        "                             transform = transform)\n",
        "mnist_test = datasets.MNIST(root = root,\n",
        "                            download = True,\n",
        "                            train = False,\n",
        "                            transform = transform)\n",
        "\n",
        "\n",
        "#学習に用いるためにデータセットをDataLoaderオブジェクトに変換\n",
        "#minibatch学習の時にバッチ単位でデータ処理できる、かつ、各epochでデータシャッフル可能\n",
        "train_dataloader = DataLoader(mnist_train, \n",
        "                              batch_size = 100,\n",
        "                              shuffle = True)\n",
        "\n",
        "test_dataloader = DataLoader(mnist_test,\n",
        "                             batch_size = 100,\n",
        "                             shuffle = False)\n",
        "\n",
        "'''\n",
        "  2 モデルの構築\n",
        "'''\n",
        "model = DNN(784, 200, 10).to(device)\n",
        "\n",
        "'''\n",
        "  3　モデルの学習\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optimizers.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "def compute_loss(t, y):\n",
        "  return criterion(y, t)\n",
        "\n",
        "def train_step(x, t):\n",
        "  model.train()\n",
        "  preds = model(x)\n",
        "  loss = compute_loss(t, preds)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss, preds\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.\n",
        "  train_acc = 0.\n",
        "\n",
        "  for(x, t) in train_dataloader:\n",
        "    x,t = x.to(device), t.to(device)\n",
        "    loss, preds = train_step(x,t)\n",
        "    train_loss += loss.item()\n",
        "    #accuracy_socreはテンソル型をうけとれないので.tolist()を実行\n",
        "    train_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "  \n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "\n",
        "  print('epoch:{}, loss: {:.3f}, a㏄:{:.3f}'.format(epoch +1,\n",
        "                                                    train_loss,\n",
        "                                                    train_acc))\n",
        "\n",
        "'''\n",
        "  4 モデルの評価\n",
        "'''\n",
        "\n",
        "def test_step(x,t):\n",
        "  model.eval()\n",
        "  preds = model(x)\n",
        "  loss = criterion(preds, t)\n",
        "  return loss, preds\n",
        "\n",
        "test_loss = 0.\n",
        "test_acc = 0.\n",
        "\n",
        "for(x,t) in test_dataloader:\n",
        "  x, t = x.to(device), t.to(device)\n",
        "  loss, preds = test_step(x,t)\n",
        "  test_loss += loss.item()\n",
        "  test_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "test_loss /= len(test_dataloader)\n",
        "test_acc /= len(test_dataloader)\n",
        "\n",
        "\n",
        "print('epoch:{}, loss: {:.3f}, acc:{:.3f}'.format(epoch +1,\n",
        "                                                    test_loss,\n",
        "                                                    test_acc))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 ReLU\n",
        "導関数が1→勾配が消失することがない<br>\n",
        "学習が早い<br>\n",
        "but<br>\n",
        "一度不活性になるとずっと不活性<br>\n",
        "学習率を大きい値に設定すると初めの誤差逆伝搬でニューロンの値が小さくなりすぎてしまう\n",
        "\n",
        "### Sigmoid,Tanhよりaccuracy上がった！！\n",
        "0.861→0.953→0.965"
      ],
      "metadata": {
        "id": "_6dXf7yqtHCB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optimizers\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''\n",
        "  (1) modelの実装\n",
        "'''\n",
        "class DNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.a1 = nn.ReLU()\n",
        "    self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.a2 = nn.ReLU()\n",
        "    self.l3 = nn.Linear(hidden_dim,hidden_dim)\n",
        "    self.a3 = nn.ReLU()\n",
        "    self.l4 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    self.layers = [self.l1, self.a1,\n",
        "                    self.l2, self.a2,\n",
        "                    self.l3, self.a3,\n",
        "                    self.l4]\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  np.random.seed(123)\n",
        "  #torch用の乱数シード\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  #deviceに実行環境を格納して同じコードでCPUでもGPUでも対応できるように\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "'''\n",
        "  1 データの準備\n",
        "'''\n",
        "root = os.path.join('~', '.torch', 'mnist')\n",
        "#numpyをTテンensorに変換し、さらにTensorの次元を(28，28)から(784、)に変換\n",
        "transform = transforms.Compose([transforms.ToTensor(), lambda x: x.view(-1)])\n",
        "\n",
        "mnist_train = datasets.MNIST(root = root,\n",
        "                             download = True,\n",
        "                             train = True,\n",
        "                             transform = transform)\n",
        "mnist_test = datasets.MNIST(root = root,\n",
        "                            download = True,\n",
        "                            train = False,\n",
        "                            transform = transform)\n",
        "\n",
        "\n",
        "#学習に用いるためにデータセットをDataLoaderオブジェクトに変換\n",
        "#minibatch学習の時にバッチ単位でデータ処理できる、かつ、各epochでデータシャッフル可能\n",
        "train_dataloader = DataLoader(mnist_train, \n",
        "                              batch_size = 100,\n",
        "                              shuffle = True)\n",
        "\n",
        "test_dataloader = DataLoader(mnist_test,\n",
        "                             batch_size = 100,\n",
        "                             shuffle = False)\n",
        "\n",
        "'''\n",
        "  2 モデルの構築\n",
        "'''\n",
        "model = DNN(784, 200, 10).to(device)\n",
        "\n",
        "'''\n",
        "  3　モデルの学習\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optimizers.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "def compute_loss(t, y):\n",
        "  return criterion(y, t)\n",
        "\n",
        "def train_step(x, t):\n",
        "  model.train()\n",
        "  preds = model(x)\n",
        "  loss = compute_loss(t, preds)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss, preds\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.\n",
        "  train_acc = 0.\n",
        "\n",
        "  for(x, t) in train_dataloader:\n",
        "    x,t = x.to(device), t.to(device)\n",
        "    loss, preds = train_step(x,t)\n",
        "    train_loss += loss.item()\n",
        "    #accuracy_socreはテンソル型をうけとれないので.tolist()を実行\n",
        "    train_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "  \n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "\n",
        "  print('epoch:{}, loss: {:.3f}, a㏄:{:.3f}'.format(epoch +1,\n",
        "                                                    train_loss,\n",
        "                                                    train_acc))\n",
        "\n",
        "'''\n",
        "  4 モデルの評価\n",
        "'''\n",
        "\n",
        "def test_step(x,t):\n",
        "  model.eval()\n",
        "  preds = model(x)\n",
        "  loss = criterion(preds, t)\n",
        "  return loss, preds\n",
        "\n",
        "test_loss = 0.\n",
        "test_acc = 0.\n",
        "\n",
        "for(x,t) in test_dataloader:\n",
        "  x, t = x.to(device), t.to(device)\n",
        "  loss, preds = test_step(x,t)\n",
        "  test_loss += loss.item()\n",
        "  test_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "test_loss /= len(test_dataloader)\n",
        "test_acc /= len(test_dataloader)\n",
        "\n",
        "\n",
        "print('epoch:{}, loss: {:.3f}, acc:{:.3f}'.format(epoch +1,\n",
        "                                                    test_loss,\n",
        "                                                    test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWDGfR6BscVT",
        "outputId": "87d859e7-3176-430b-9a89-4e70524c55fc"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, loss: 2.273, a㏄:0.242\n",
            "epoch:2, loss: 1.764, a㏄:0.617\n",
            "epoch:3, loss: 0.690, a㏄:0.815\n",
            "epoch:4, loss: 0.462, a㏄:0.871\n",
            "epoch:5, loss: 0.391, a㏄:0.888\n",
            "epoch:6, loss: 0.355, a㏄:0.898\n",
            "epoch:7, loss: 0.330, a㏄:0.905\n",
            "epoch:8, loss: 0.309, a㏄:0.911\n",
            "epoch:9, loss: 0.291, a㏄:0.917\n",
            "epoch:10, loss: 0.275, a㏄:0.921\n",
            "epoch:11, loss: 0.259, a㏄:0.926\n",
            "epoch:12, loss: 0.245, a㏄:0.930\n",
            "epoch:13, loss: 0.232, a㏄:0.934\n",
            "epoch:14, loss: 0.220, a㏄:0.938\n",
            "epoch:15, loss: 0.208, a㏄:0.940\n",
            "epoch:16, loss: 0.198, a㏄:0.943\n",
            "epoch:17, loss: 0.188, a㏄:0.946\n",
            "epoch:18, loss: 0.179, a㏄:0.949\n",
            "epoch:19, loss: 0.170, a㏄:0.951\n",
            "epoch:20, loss: 0.162, a㏄:0.953\n",
            "epoch:21, loss: 0.155, a㏄:0.955\n",
            "epoch:22, loss: 0.148, a㏄:0.958\n",
            "epoch:23, loss: 0.142, a㏄:0.959\n",
            "epoch:24, loss: 0.135, a㏄:0.961\n",
            "epoch:25, loss: 0.130, a㏄:0.962\n",
            "epoch:26, loss: 0.124, a㏄:0.964\n",
            "epoch:27, loss: 0.119, a㏄:0.965\n",
            "epoch:28, loss: 0.114, a㏄:0.967\n",
            "epoch:29, loss: 0.110, a㏄:0.968\n",
            "epoch:30, loss: 0.106, a㏄:0.969\n",
            "epoch:30, loss: 0.114, acc:0.965\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Leaky ReLU\n",
        "f(x) = max(αx, x)\n",
        "x < 0でも学習が進む！！ \n",
        "### accuracy！！\n",
        "0.861→0.953→0.965→0.964"
      ],
      "metadata": {
        "id": "mtsY6x6sv1uH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optimizers\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''\n",
        "  (1) modelの実装\n",
        "'''\n",
        "class DNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.a1 = nn.LeakyReLU(0.01)\n",
        "    self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.a2 = nn.LeakyReLU(0.01)\n",
        "    self.l3 = nn.Linear(hidden_dim,hidden_dim)\n",
        "    self.a3 = nn.LeakyReLU(0.01)\n",
        "    self.l4 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    self.layers = [self.l1, self.a1,\n",
        "                    self.l2, self.a2,\n",
        "                    self.l3, self.a3,\n",
        "                    self.l4]\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  np.random.seed(123)\n",
        "  #torch用の乱数シード\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  #deviceに実行環境を格納して同じコードでCPUでもGPUでも対応できるように\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "'''\n",
        "  1 データの準備\n",
        "'''\n",
        "root = os.path.join('~', '.torch', 'mnist')\n",
        "#numpyをTテンensorに変換し、さらにTensorの次元を(28，28)から(784、)に変換\n",
        "transform = transforms.Compose([transforms.ToTensor(), lambda x: x.view(-1)])\n",
        "\n",
        "mnist_train = datasets.MNIST(root = root,\n",
        "                             download = True,\n",
        "                             train = True,\n",
        "                             transform = transform)\n",
        "mnist_test = datasets.MNIST(root = root,\n",
        "                            download = True,\n",
        "                            train = False,\n",
        "                            transform = transform)\n",
        "\n",
        "\n",
        "#学習に用いるためにデータセットをDataLoaderオブジェクトに変換\n",
        "#minibatch学習の時にバッチ単位でデータ処理できる、かつ、各epochでデータシャッフル可能\n",
        "train_dataloader = DataLoader(mnist_train, \n",
        "                              batch_size = 100,\n",
        "                              shuffle = True)\n",
        "\n",
        "test_dataloader = DataLoader(mnist_test,\n",
        "                             batch_size = 100,\n",
        "                             shuffle = False)\n",
        "\n",
        "'''\n",
        "  2 モデルの構築\n",
        "'''\n",
        "model = DNN(784, 200, 10).to(device)\n",
        "\n",
        "'''\n",
        "  3　モデルの学習\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optimizers.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "def compute_loss(t, y):\n",
        "  return criterion(y, t)\n",
        "\n",
        "def train_step(x, t):\n",
        "  model.train()\n",
        "  preds = model(x)\n",
        "  loss = compute_loss(t, preds)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss, preds\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.\n",
        "  train_acc = 0.\n",
        "\n",
        "  for(x, t) in train_dataloader:\n",
        "    x,t = x.to(device), t.to(device)\n",
        "    loss, preds = train_step(x,t)\n",
        "    train_loss += loss.item()\n",
        "    #accuracy_socreはテンソル型をうけとれないので.tolist()を実行\n",
        "    train_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "  \n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "\n",
        "  print('epoch:{}, loss: {:.3f}, a㏄:{:.3f}'.format(epoch +1,\n",
        "                                                    train_loss,\n",
        "                                                    train_acc))\n",
        "\n",
        "'''\n",
        "  4 モデルの評価\n",
        "'''\n",
        "\n",
        "def test_step(x,t):\n",
        "  model.eval()\n",
        "  preds = model(x)\n",
        "  loss = criterion(preds, t)\n",
        "  return loss, preds\n",
        "\n",
        "test_loss = 0.\n",
        "test_acc = 0.\n",
        "\n",
        "for(x,t) in test_dataloader:\n",
        "  x, t = x.to(device), t.to(device)\n",
        "  loss, preds = test_step(x,t)\n",
        "  test_loss += loss.item()\n",
        "  test_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "test_loss /= len(test_dataloader)\n",
        "test_acc /= len(test_dataloader)\n",
        "\n",
        "\n",
        "print('epoch:{}, loss: {:.3f}, acc:{:.3f}'.format(epoch +1,\n",
        "                                                    test_loss,\n",
        "                                                    test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XIT6VUf3vA7U",
        "outputId": "69add6ca-578c-4f51-eadd-57fc8bbdeacf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, loss: 2.273, a㏄:0.245\n",
            "epoch:2, loss: 1.756, a㏄:0.620\n",
            "epoch:3, loss: 0.686, a㏄:0.816\n",
            "epoch:4, loss: 0.461, a㏄:0.871\n",
            "epoch:5, loss: 0.390, a㏄:0.888\n",
            "epoch:6, loss: 0.355, a㏄:0.898\n",
            "epoch:7, loss: 0.329, a㏄:0.905\n",
            "epoch:8, loss: 0.309, a㏄:0.912\n",
            "epoch:9, loss: 0.291, a㏄:0.917\n",
            "epoch:10, loss: 0.275, a㏄:0.921\n",
            "epoch:11, loss: 0.260, a㏄:0.925\n",
            "epoch:12, loss: 0.246, a㏄:0.929\n",
            "epoch:13, loss: 0.233, a㏄:0.934\n",
            "epoch:14, loss: 0.221, a㏄:0.937\n",
            "epoch:15, loss: 0.209, a㏄:0.940\n",
            "epoch:16, loss: 0.199, a㏄:0.943\n",
            "epoch:17, loss: 0.189, a㏄:0.945\n",
            "epoch:18, loss: 0.180, a㏄:0.948\n",
            "epoch:19, loss: 0.171, a㏄:0.951\n",
            "epoch:20, loss: 0.163, a㏄:0.953\n",
            "epoch:21, loss: 0.156, a㏄:0.955\n",
            "epoch:22, loss: 0.149, a㏄:0.958\n",
            "epoch:23, loss: 0.143, a㏄:0.959\n",
            "epoch:24, loss: 0.136, a㏄:0.961\n",
            "epoch:25, loss: 0.131, a㏄:0.962\n",
            "epoch:26, loss: 0.125, a㏄:0.964\n",
            "epoch:27, loss: 0.120, a㏄:0.965\n",
            "epoch:28, loss: 0.115, a㏄:0.967\n",
            "epoch:29, loss: 0.111, a㏄:0.968\n",
            "epoch:30, loss: 0.107, a㏄:0.969\n",
            "epoch:30, loss: 0.114, acc:0.964\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Swish\n",
        "f(x) = x σ(βx) <br>\n",
        "全範囲で微分可能<br>\n",
        "but<br>\n",
        "計算コスト"
      ],
      "metadata": {
        "id": "o3imxZMxxkj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import torch\n",
        "import torch.nn as nn \n",
        "import torch.optim as optimizers\n",
        "from torchvision import datasets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "'''\n",
        "  (1) modelの実装\n",
        "'''\n",
        "class Swish(nn.Module):\n",
        "  #nn.Moduleを継承\n",
        "  def __init__(self, beta = 1.):\n",
        "    super().__init__()\n",
        "    self.beta = beta\n",
        "  def forward(self, x):\n",
        "    return x * torch.sigmoid(self.beta*x)\n",
        "\n",
        "class DNN(nn.Module):\n",
        "  def __init__(self, input_dim, hidden_dim, output_dim):\n",
        "    super().__init__()\n",
        "    self.l1 = nn.Linear(input_dim, hidden_dim)\n",
        "    self.a1 = Swish()\n",
        "    self.l2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "    self.a2 = Swish()\n",
        "    self.l3 = nn.Linear(hidden_dim,hidden_dim)\n",
        "    self.a3 = Swish()\n",
        "    self.l4 = nn.Linear(hidden_dim, output_dim)\n",
        "    \n",
        "    self.layers = [self.l1, self.a1,\n",
        "                    self.l2, self.a2,\n",
        "                    self.l3, self.a3,\n",
        "                    self.l4]\n",
        "\n",
        "  def forward(self,x):\n",
        "    for layer in self.layers:\n",
        "      x = layer(x)\n",
        "    return x\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  np.random.seed(123)\n",
        "  #torch用の乱数シード\n",
        "  torch.manual_seed(123)\n",
        "\n",
        "  #deviceに実行環境を格納して同じコードでCPUでもGPUでも対応できるように\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "'''\n",
        "  1 データの準備\n",
        "'''\n",
        "root = os.path.join('~', '.torch', 'mnist')\n",
        "#numpyをTテンensorに変換し、さらにTensorの次元を(28，28)から(784、)に変換\n",
        "transform = transforms.Compose([transforms.ToTensor(), lambda x: x.view(-1)])\n",
        "\n",
        "mnist_train = datasets.MNIST(root = root,\n",
        "                             download = True,\n",
        "                             train = True,\n",
        "                             transform = transform)\n",
        "mnist_test = datasets.MNIST(root = root,\n",
        "                            download = True,\n",
        "                            train = False,\n",
        "                            transform = transform)\n",
        "\n",
        "\n",
        "#学習に用いるためにデータセットをDataLoaderオブジェクトに変換\n",
        "#minibatch学習の時にバッチ単位でデータ処理できる、かつ、各epochでデータシャッフル可能\n",
        "train_dataloader = DataLoader(mnist_train, \n",
        "                              batch_size = 100,\n",
        "                              shuffle = True)\n",
        "\n",
        "test_dataloader = DataLoader(mnist_test,\n",
        "                             batch_size = 100,\n",
        "                             shuffle = False)\n",
        "\n",
        "'''\n",
        "  2 モデルの構築\n",
        "'''\n",
        "model = DNN(784, 200, 10).to(device)\n",
        "\n",
        "'''\n",
        "  3　モデルの学習\n",
        "'''\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optimizers.SGD(model.parameters(), lr = 0.01)\n",
        "\n",
        "def compute_loss(t, y):\n",
        "  return criterion(y, t)\n",
        "\n",
        "def train_step(x, t):\n",
        "  model.train()\n",
        "  preds = model(x)\n",
        "  loss = compute_loss(t, preds)\n",
        "  optimizer.zero_grad()\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "  return loss, preds\n",
        "\n",
        "epochs = 30\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  train_loss = 0.\n",
        "  train_acc = 0.\n",
        "\n",
        "  for(x, t) in train_dataloader:\n",
        "    x,t = x.to(device), t.to(device)\n",
        "    loss, preds = train_step(x,t)\n",
        "    train_loss += loss.item()\n",
        "    #accuracy_socreはテンソル型をうけとれないので.tolist()を実行\n",
        "    train_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "  \n",
        "  train_loss /= len(train_dataloader)\n",
        "  train_acc /= len(train_dataloader)\n",
        "\n",
        "\n",
        "  print('epoch:{}, loss: {:.3f}, a㏄:{:.3f}'.format(epoch +1,\n",
        "                                                    train_loss,\n",
        "                                                    train_acc))\n",
        "\n",
        "'''\n",
        "  4 モデルの評価\n",
        "'''\n",
        "\n",
        "def test_step(x,t):\n",
        "  model.eval()\n",
        "  preds = model(x)\n",
        "  loss = criterion(preds, t)\n",
        "  return loss, preds\n",
        "\n",
        "test_loss = 0.\n",
        "test_acc = 0.\n",
        "\n",
        "for(x,t) in test_dataloader:\n",
        "  x, t = x.to(device), t.to(device)\n",
        "  loss, preds = test_step(x,t)\n",
        "  test_loss += loss.item()\n",
        "  test_acc += accuracy_score(t.tolist(), preds.argmax(dim = -1).tolist())\n",
        "test_loss /= len(test_dataloader)\n",
        "test_acc /= len(test_dataloader)\n",
        "\n",
        "\n",
        "print('epoch:{}, loss: {:.3f}, acc:{:.3f}'.format(epoch +1,\n",
        "                                                    test_loss,\n",
        "                                                    test_acc))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8IwnZd0Lxfan",
        "outputId": "4611b9d3-3e45-4825-ff99-907a888790b4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epoch:1, loss: 2.300, a㏄:0.123\n",
            "epoch:2, loss: 2.289, a㏄:0.180\n",
            "epoch:3, loss: 2.268, a㏄:0.261\n",
            "epoch:4, loss: 2.154, a㏄:0.291\n",
            "epoch:5, loss: 1.684, a㏄:0.518\n",
            "epoch:6, loss: 0.819, a㏄:0.759\n",
            "epoch:7, loss: 0.559, a㏄:0.840\n",
            "epoch:8, loss: 0.454, a㏄:0.872\n",
            "epoch:9, loss: 0.405, a㏄:0.885\n",
            "epoch:10, loss: 0.377, a㏄:0.893\n",
            "epoch:11, loss: 0.356, a㏄:0.898\n",
            "epoch:12, loss: 0.340, a㏄:0.904\n",
            "epoch:13, loss: 0.326, a㏄:0.908\n",
            "epoch:14, loss: 0.314, a㏄:0.910\n",
            "epoch:15, loss: 0.303, a㏄:0.914\n",
            "epoch:16, loss: 0.294, a㏄:0.917\n",
            "epoch:17, loss: 0.285, a㏄:0.918\n",
            "epoch:18, loss: 0.278, a㏄:0.920\n",
            "epoch:19, loss: 0.271, a㏄:0.923\n",
            "epoch:20, loss: 0.264, a㏄:0.924\n",
            "epoch:21, loss: 0.260, a㏄:0.925\n",
            "epoch:22, loss: 0.254, a㏄:0.927\n",
            "epoch:23, loss: 0.249, a㏄:0.928\n",
            "epoch:24, loss: 0.243, a㏄:0.930\n",
            "epoch:25, loss: 0.237, a㏄:0.932\n",
            "epoch:26, loss: 0.232, a㏄:0.934\n",
            "epoch:27, loss: 0.227, a㏄:0.934\n",
            "epoch:28, loss: 0.221, a㏄:0.936\n",
            "epoch:29, loss: 0.215, a㏄:0.938\n",
            "epoch:30, loss: 0.210, a㏄:0.940\n",
            "epoch:30, loss: 0.211, acc:0.938\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "FTiGmfTzzdpi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}